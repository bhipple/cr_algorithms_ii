// ============================================================================
//                                  Week 1
// ============================================================================
Greedy Algorithms:
    Optimal Weighted Average completion time Job Scheduling
    Prim's MST

    Exchange argument proof technique

// ============================================================================
//                                  Week 2
// ============================================================================
Prim's MST Algorithm, Kruskal's MST Algorithm

Union Find Data Structure
    Maintains a partition of a set of objects
    Find(x): returns the name Ci of the set to which x belongs
    Union(C1, C2): Fuse all x in C1 and y in C2 to form one set, C3

    Every element x has a pointer to the leader of its cluster
    Find(x) runs in O(1) time by just returning the name of the node at the pointer

    Union(C1, C2) runs in O(log(n)) time, with the following strategy:
        Every Ci keeps a field with the number of elements in its cluster
        WLOG assume |C1| <= |C2|:
            Then we take all x in C1 and update their leader to be the leader of any element in C2
            This is at worst an O(n) operation, but a given node only needs to have its leader pointer
            changed at most log(n) times, since it only gets changed when it's in the smaller group.

    Using this to implement Kruskal's algorithm, we find:
    O(mlogn) for preprocessing sorting
    O(m) time for the m O(1) cycle checks
    O(nlogn) time for the leader pointer updates

    ==> O(mlogn) total time

MST State of the Art Algorithms
    Karger-Klein-Tarjan JACM (1995):
        O(m) average randomized algorithm (non-deterministic)

    Chazelle Jotem (2000):
        O(m * alpha(n)) deterministic runtime solution
        where alpha(n) is the inverse Ackermann function, which grows *extremely* slowly

Clustering
    Let k be the number of clusters, define separated points as points p, q in different clusters
    Max-Spacing k-Clusterings:
        Given a distance measure and k, find the clustering which maximizes the spacing
        between the clusters.  That is, we want to maximize the spacing between the closest
        separated points.

    Hierarchical Agglomerative Clustering - each point starts in its own cluster, and we repeatedly
    merge the two closest clusters until we have only k clusters remaining.  This greedy clustering
    algorithm is exactly the same as Kruskal's MST algorithm, except we abort early!

    Also called single-link clustering

Huffman Codes
    We want to map each character of an alphabet to a binary string, using variable length encodings
    This allows us to save space when some characters are more frequent than others

    Prefix-Free codes: for every pair i, j in the alphabet, neither of the encodings f(i) or f(j)
    is a prefix of the other. This avoids interpretation ambiguity.

    Input: An alphabet Sigma, along with a probability p for each i in Sigma
    Output: A binary tree with leaves <-> symbols of Sigma, minimizing
    the average encoding length:
        L(T) = sum(p_i * depth-of-i-in-Tree)

    Greedy step: We take the two smallest nodes i,j  and merge them together into a new
    node whose weight is p(i) + p(j), under an internal node

    Repeat until we only have two nodes, at which point we add an internal node at the root
    with left branch (0) going to i and right branch (1) going to j

Advanced Union-Find
    If we consider merging two clusters, with leaders A and B, instead of rewiring all
    nodes in cluster B to point to A, we can just re-wire leader of B to point to A
    This is a lazy union approach, and is much more efficient than the one previously discussed

    We can implement this way an array X, so that X[i] gives the number of the leader
    of element i.  Initially, X[i] = i for all i.

    Then for any node i we can find its parent with this recursive algo:
    def findLeader(i):
        if X[i] = i:
            return i
        return findLeader(X[i])

    This approach makes the unions take constant time (instead of log(n) time), but now
    the find operations are more expensive -- up to log(n) time!

    When we have the two clusters and we have to decide which one becomes the parent, we use a technique called
    Union By Rank
        For each x in X, maintain field rank[x]
        Invariant: rank[x] = maximum number of hops from some leaf to x (height of x's subtree)

// ============================================================================
//                                  Week 3
// ============================================================================
Dynamic Programming

Weighted Independent Sets in Path Graphs
    Given a path graph (a line), where each vertex has a weight,
    choose the subset of the nodes that maximizes the total weight of
    the subset under the constraint that the set does not contain any
    adjacent vertices

    Let G = a path graph with n weighted vertices.
    Consider an optimal solution S in G.  Then S either has or does
    not have the last vertex v(n).  If it does not have v(n), then
    S is also an optimal solution to the subgraph G' = G - {v(n)}
    In this case we can remove the last vertex and repeat.

    If v(n) is in S, then S' = S - {v(n)} is an optimal solution
    for the subgraph G'' = G - { v(n), v(n-1) }. In this case, we
    can recurse on G''!  We don't know which of these two conditions
    we're in, so we'll have to recursively try both.

    If we do this recursion naively, it's O(2^n) complexity, but with
    dynamic programming we can reduce it to O(n)

    i.e.,
    Recursively compute:
    S1 = max-wt IS of G'
    S2 = max-wt IS of G''
    return max(S1, S2 U {v(n)})

    This recursion does O(2^n) recursive calls, but only on O(n) distinct problems,
    so we can just memoize results.

    Alternatively, we can phrase it in an iterative bottom-up approach.  We populate
    an array A left to right such that A[i] = value of max-wt IS of Gi.
    Initialize A[0] = 0; A[1] = w1.
    For each i, A[i] = max(A[i-1], A[i-2] + wi
    This algorithm tells us the optimal value

    We may also be interested in the optimal set S. We can trace back through the filled-in
    array to reconstruct S without doing any extra storage or computation during the algo!

    i = len(A)
    while i >= 1:
        if A[i-1] >= A[i-2] + w(i):
            i -= 1
        else:
            S.add(i)
            i -= 2

    General Principles of Dynamic Programming
        1) Identify a small number of subproblems
        2) can quickly + correctly solve "larger" subproblems given
        the solution to "smaller subproblems", usually via a recurrence
        3) After solving all subproblems, can quickly compute the final solution.
        Often, the final solution is just the "biggest" subproblem

Knapsack Problem
    Given n items, each with a non-negative value v(i), and a non-negative integral size w(i),
    and a nonnegative integer W for the size of the knapsack

    Find the set S with maximal value under the constraint that the sum of weights of elements
    in S is <= W

    Consider an optional set S for a set of n items.  If item n is in S, then S - {n} is an
    optimal solution for the first (n-1) items for capacity W - w(n)

    Given a position, the subproblems are:
        All possible prefixes of items {1, 2, ..., i}
        All possible integral residual capacities x in {0,1,2,...,W}

    Let A = a 2-D array
    Initialize A[0, x] = 0 for x = o, 1, 2, . . ., W
    for i in range(n+1):
        for x in range(W+1):
            A[i,x] = max(A[i-1, x], A[i-1, x - w(i)])  # Assuming w(i) fits in the knapsack

Optimal Binary Search Trees
    When item searching is unformly distributed, a balanced binary search tree is fine.
    When items have different search frequencies, though, we can often do better with
    a carefully constructed BST that still satisfies the BST property but is not necessarily
    balanced.

    Given a collection of elements i in T and a p(i) for the probability of key i being
    searched for, construct a BST that minimizes C(T) = sum(p(i) * search-time-for-i-in-T)

// ============================================================================
//                                  Week 4
// ============================================================================
Bellman Ford Algorithm for shortest paths in graphs with negative costs

Let A = [][], indexed by i and v
Base case:
    When i = 0, then A[0, s] = 0; A[0,v] = infi for all v not equal to s

for i = 1 to n-1:
    for each v in V:
        A[i,v] = min(A[i-1, v], min (w,v in E) { A[i-1, w] + e(wv) }
        # That is, the path from s to v that uses at most i edges is
        # the minimum of the path from s to v that uses at most i-1 edges
        # and, for every w in V such that there is an edge from w to v,
        # the minimum of every i-1 edge path from s to w + the weight
        # of the edge (wv)

All Pairs Shortest Paths
Given n vertices, compute the shortest path between all pairs of vertices
    If the graph is sparese and has no negative edges, we can get away with
    running Dijkstra's algorithm n times, for a runtime of
    O(n * m * log(m)) --> O(n^2 * log(n))
    If the graph is dense, then we're looking at O(n^3 * log(n))

    If there are negative edges, we could run Bellman Ford n times, but its running
    time is O(n^2 * m), so we're at cubic time if the graph is sparese and
    quartic time if the graph is dense.

    Floyd Warshall
        O(n^3) algorithm for APSP, even with negative edges and dense graphs
        In sparse graphs, this is at least as good as Bellman-Ford, but it's
        much better in dense graphs.
        In graphs with non-negative edges, we usually just want to run Dijkstra

    Base initialization:
    Let A = [i,j,k] be a 3D array
    for all i,j in V: A[i,j,0] = { 0 if i = j,
                                   c(ij) if (i,j) in E,
                                   infi if i != j and (i,j) not in E}

    Run:
    for k = 1 to n
        for i = 1 to n
            for j = 1 to n
                A[i,j,k] = min { A[i,j,k-1
                                 A[i,k,k-1] + A[k,j,k-1]
                               }

    Once it finishes, we can scan the diagonal of A to check if
    G had a negative cycle.  If we see an A[i,i,n] < 0 for at least one i,
    then there was a negative cycle. Essentially, this means that there
    is a path to a node that is shorter than the 0-length path. This will
    always happen when there's a cycle for obvious reasons.

    If we want to reconstruct the actual path, and not just the cost, we
    will need to store an extra constant space per subproblem.
    B[i,j] = max label of an internal node on a shortest i-j path for all i,j in V

    Now if we want to find the shortest path between i and j, we will look at
    B[i,j] = x.  This says that the shortest path from i to j is the shortest path
    from i to x + the shortest path from x to j. We can then recursively look at
    B[i,x] and B[x,j] to fill in the path until we terminate with a completed path.


    Johnson's Algorithm for APSP
    It turns out we can get away in general graphs with running 1 invocation of
    Bellman Ford, and then n-1 iterators of Dijkstra's algorithm.

    For the BF run, we add a new vertex s which is connected to every node with an
    edge cost of 0. We run BF starting at this s, and find the shortest path to every
    other node. We then give every vertex a weight according to its shortest s-v path
    length.  We can then re-weight every (i,j) edge to be equal to c(i,j) + w(i) - w(j)
    This reweighting will eliminate all negative edges without impacting the shortest
    paths.
